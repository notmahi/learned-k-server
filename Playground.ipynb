{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 2\n",
    "num_servers = 3\n",
    "batch_size = 15\n",
    "avg_over = 100\n",
    "eps_greedy = 0.5\n",
    "alpha = 0.5\n",
    "gamma = 0.8\n",
    "\n",
    "def model(t):\n",
    "    batch = t.shape[0]\n",
    "    num_servers = t.shape[1] - 1\n",
    "    dim = t.shape[2]\n",
    "    \n",
    "    return torch.rand((batch, num_servers))\n",
    "\n",
    "def sample_req(n):\n",
    "    return torch.rand((n, dim))\n",
    "  \n",
    "def compute_distance(reqs, servers):\n",
    "    return ((reqs - servers) ** 2).sum(dim=-1) ** 0.5\n",
    "\n",
    "def train():\n",
    "    train_batch = sample_req(batch_size * (num_servers + 1))\n",
    "    train_batch = train_batch.reshape(batch_size, (num_servers + 1), dim)\n",
    "    \n",
    "    q_value_old = model(train_batch)\n",
    "    best_indices = q_value_old.argmax(dim=1)\n",
    "    rand_indices = torch.randint_like(best_indices, low=0, high=num_servers)\n",
    "    index_picking = torch.bernoulli( eps_greedy * torch.ones_like(best_indices).double() ).long()\n",
    "    \n",
    "    indices = (index_picking * rand_indices) + ((1. - index_picking) * best_indices)\n",
    "    print(indices)\n",
    "    \n",
    "    new_locations = train_batch.clone().detach()\n",
    "    location_to_move_to = new_locations[:, -1, :]\n",
    "    # print(new_locations[:, indices, :])\n",
    "    old_server_loc = new_locations[range(batch_size), indices, :]\n",
    "    batch_distance = compute_distance(old_server_loc, location_to_move_to)\n",
    "    new_locations[range(batch_size), indices, :] = location_to_move_to\n",
    "    # new_locations = new_locations[:, :-1 ,:]\n",
    "    \n",
    "    new_req = sample_req(avg_over)\n",
    "    \n",
    "    locations_to_avg_over = new_locations[ [i for i in range(batch_size) for j in range(avg_over)], :, :]\n",
    "    new_req_multiplied = new_req[[i for i in range(batch_size) for j in range(avg_over)], :]\n",
    "    \n",
    "    locations_to_avg_over[:, -1, :] = new_req_multiplied\n",
    "    \n",
    "    q_values_to_avg = model(locations_to_avg_over)\n",
    "    max_q_value_to_avg = q_values_to_avg.max(dim=1)[0]\n",
    "    max_q_values = max_q_value_to_avg.reshape(batch_size, avg_over).mean(dim=1)\n",
    "    \n",
    "    update_values = alpha * ( -batch_distance + gamma * max_q_values)\n",
    "    q_value_new = q_value_old.clone().detach()\n",
    "    q_value_new[range(batch_size), indices] = (1 - alpha) * q_value_new[range(batch_size), indices] + update_values\n",
    "    \n",
    "    print(q_value_old)\n",
    "    print(q_value_new)\n",
    "    loss = ((q_value_old - q_value_new) ** 2).sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 2, 2, 2])\n",
      "tensor([[0.3764, 0.7767, 0.4091],\n",
      "        [0.5342, 0.4344, 0.0927],\n",
      "        [0.7355, 0.8481, 0.9093],\n",
      "        [0.6222, 0.5061, 0.2923],\n",
      "        [0.4681, 0.5915, 0.5471],\n",
      "        [0.7296, 0.0865, 0.6821],\n",
      "        [0.1558, 0.6061, 0.4681],\n",
      "        [0.4176, 0.8549, 0.8407],\n",
      "        [0.6358, 0.7394, 0.5939],\n",
      "        [0.2773, 0.8519, 0.6902],\n",
      "        [0.8763, 0.6501, 0.2752],\n",
      "        [0.4381, 0.7223, 0.8781],\n",
      "        [0.1106, 0.1261, 0.3758],\n",
      "        [0.2459, 0.3104, 0.4005],\n",
      "        [0.3211, 0.6888, 0.7006]])\n",
      "tensor([[ 0.3764,  0.6428,  0.4091],\n",
      "        [ 0.3435,  0.4344,  0.0927],\n",
      "        [ 0.4641,  0.8481,  0.9093],\n",
      "        [ 0.5424,  0.5061,  0.2923],\n",
      "        [ 0.4681,  0.3605,  0.5471],\n",
      "        [ 0.3621,  0.0865,  0.6821],\n",
      "        [-0.0413,  0.6061,  0.4681],\n",
      "        [ 0.2816,  0.8549,  0.8407],\n",
      "        [ 0.1434,  0.7394,  0.5939],\n",
      "        [ 0.2773,  0.2829,  0.6902],\n",
      "        [ 0.5116,  0.6501,  0.2752],\n",
      "        [ 0.4381,  0.7223,  0.6719],\n",
      "        [ 0.1106,  0.1261,  0.2291],\n",
      "        [ 0.2459,  0.3104,  0.1469],\n",
      "        [ 0.3211,  0.6888,  0.3435]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-6e91aa3dfe3c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_value_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mq_value_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/scratch/nshafiul/dl/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/scratch/nshafiul/dl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  3.6056,  6.4031,  9.2195, 12.0416])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(10)\n",
    "t = t.reshape(5, 2).float()\n",
    "torch.norm(t, p=2, dim=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
